#' Evaluate model on training data
#'
#' `HIP_train_eval` is used to get training metrics (e.g. MSE or classification accuracy)
#' from a model built using HIP. It can be used with models generated by `select_lambda` or
#' `fixed_lambda`.
#'
#' @usage HIP_train_eval(data, res, fix_or_search)
#'
#' @param data data object such as from `generate_data` or `py_data`
#' @param res training result such as from `select_lambda` or `fixed_lambda`
#' @param fix_or_search string - 'fixed' for results from `fixed_lambda`,
#' 'search' for results from `select_lambda`
#'
#' @returns `HIP_train_eval` returns a list with the following:
#' \item{`each`}{tensor(double) - (for gaussian outcomes) tensor of MSEs for each of the q outcomes}
#' \item{`comp_val`}{tensor(double) - MSE for gaussian outcomes, classification accuracy for multiclass outcomes, fraction of deviance explained for poisson/ZIP outcomes}
#' \item{`pred`}{list<tensor(double) - predicted values from fitted model. For multiclass outcomes, single integer to represent class, starting with 0. For poisson/ZIP, predicted means from fitted model}
#' \item{`dev_sum`}{list(tensor(double)) - (for poisson/ZIP outcomes), list with more detailed deviance information}
#' Also returns a histogram plot for predicted vs. true values in each subgroup.
#'
#' @examples
#' # Generate data
#' dat_gen_test <- generate_data(test_data=TRUE)
#'
#' # Get results from select_lambda
#' res <- select_lambda(dat_gen_test$X, dat_gen_test$Y, c(1,1), 'gaussian', 50,
#'                      K = 2, num_steps=c(4,4))
#'
#' # Evaluating training data
#' train_eval <- HIP_train_eval(dat_gen_test, res, 'search')
#'
#' @importFrom ggplot2 ggplot aes facet_grid vars geom_bar geom_histogram theme_classic labs


HIP_train_eval <-function(data, res, fix_or_search){

  # Import Python functions
  reticulate::source_python(system.file("python/all_functions.py",
                                        package = "HIP"))
  np <- reticulate::import("numpy")

  # standardize train data, but only standardize Y if gaussian
  if(res$family == "gaussian" & res$standardize != 'none'){
    xlab <- paste0("Standardized ", data$outcome_var)
    train_std <- standardize_dat(Y = data$Y, standardize = res$standardize, std_type = "scale_center", std_y = T)
  }else{
    xlab <- data$outcome_var
    train_std <- list(Y = data$Y)
  }

  if(fix_or_search == 'fixed'){
    res_obj <- res$out
  } else {
    res_obj <- res$out$search_results[[res$best_index]]
  }

  err <- switch(res$family,
                "gaussian" = train_mse(Y = train_std$Y,
                                          Z = res_obj$subset$Z,
                                          theta_dict = list('theta' = res_obj$subset$theta,
                                                            'beta' = res_obj$subset$beta)),
                "multiclass" = train_class(Y = train_std$Y,
                                              Z = res_obj$subset$Z,
                                              theta_dict = list('theta' = res_obj$subset$theta,
                                                                'beta' = res_obj$subset$beta)),
                "poisson" = train_pois(Y = train_std$Y,
                                          Z = res_obj$subset$Z,
                                          theta_dict = list('theta' = res_obj$subset$theta,
                                                            'beta' = res_obj$subset$beta)),
                "zip" = train_zip(Y = train_std$Y,
                                     Z = res_obj$subset$Z,
                                     theta_dict = list('theta' = res_obj$subset$theta,
                                                       'beta' = res_obj$subset$beta,
                                                       'tau' = res_obj$subset$tau))
  )

  # Format data for plotting; y_val, subgroup
  if(res$family == "multiclass"){
    # which.max will give 1 or 2, but generated data has 0 or 1
    # predicted values from Python will start at 0
    truedat <- lapply(1:data$S, FUN = function(s){
      data.frame(y_val = apply(np$array(data$Y[[s]]), 1, which.max) - 1,
                 type = "True Value",
                 subgroup = data$sub_vec[[s]])
    }) %>%
      dplyr::bind_rows()
  } else {
    truedat <- lapply(1:data$S, FUN = function(s){
      data.frame(y_val = np$array(train_std$Y[[s]])[,1],
                 type = "True Value",
                 subgroup = data$sub_vec[[s]])
    }) %>%
      dplyr::bind_rows()
  }

  preddat <- lapply(1:data$S, FUN = function(s){
    data.frame(y_val = np$array(err$pred[[s]]),
               type = "Predicted Value",
               subgroup = data$sub_vec[[s]])
  }) %>%
    dplyr::bind_rows()

  plotdat <- dplyr::bind_rows(truedat, preddat)

  if(res$family == "multiclass"){
    p <- ggplot(plotdat, aes(x = y_val)) +
      facet_grid(cols = vars(subgroup), rows = vars(type)) +
      geom_bar() +
      theme_classic() +
      labs(x = xlab,
           y = "Count")
  } else {
    p <- ggplot(plotdat, aes(x = y_val)) +
      facet_grid(cols = vars(subgroup), rows = vars(type)) +
      geom_histogram(binwidth = 1) +
      theme_classic() +
      labs(x = xlab,
           y = "Count")
  }

  print(p) # output plot to console

  return(list(err = err, plot=p))
}
#' Evaluate model on test data
#'
#' `HIP_test_eval` is used to get model performance metrics (e.g. MSE or classification accuracy)
#' on test data. It can be used with models generated by `select_lambda` or `fixed_lambda`.
#'
#' @usage HIP_test_eval(data, res, fix_or_search, outcome_var=NULL)
#'
#' @param data data object such as from `generate_data` or `py_data` containing both X, Y data and X_test, Y_test data
#' @param res training result such as from `select_lambda` or `fixed_lambda`
#' @param fix_or_search string - 'fixed' for results from `fixed_lambda`,
#' 'search' for results from `select_lambda`
#' @param outcome_var string - label of outcome variable, if available
#'
#' @returns `HIP_test_eval` returns a list with the following:
#' \item{`each`}{tensor(double) - (for gaussian outcomes) tensor of MSEs for each of the q outcomes}
#' \item{`comp_val`}{tensor(double) - MSE for gaussian outcomes, classification accuracy for multiclass outcomes, fraction of deviance explained for poisson/ZIP outcomes}
#' \item{`pred`}{list<tensor(double) - predicted values from fitted model. For multiclass outcomes, single integer to represent class, starting with 0. For poisson/ZIP, predicted means from fitted model}
#' \item{`dev_sum`}{list(tensor(double)) - (for poisson/ZIP outcomes), list with more detailed deviance information}
#'  Also returns a histogram plot for predicted vs. true values in each subgroup.
#'
#' @examples
#' # Generate data
#' dat_gen_test <- generate_data(test_data=TRUE)
#'
#' # Get results from select_lambda
#' res <- select_lambda(dat_gen_test$X, dat_gen_test$Y, c(1,1), 'gaussian', 50,
#'                      K = 2, num_steps=c(4,4))
#'
#' # Evaluating on test data
#' test_eval <- HIP_test_eval(dat_gen_test, res, 'search')
#'
#' @importFrom ggplot2 ggplot aes facet_grid vars geom_bar geom_histogram theme_classic labs


HIP_test_eval <- function(data, res, fix_or_search, outcome_var=NULL){

  # Import Python functions
  reticulate::source_python(system.file("python/all_functions.py",
                                        package = "HIP"))
  np <- reticulate::import("numpy")

  ## If test output is supplied, format it correctly
  if(!is.null(data$Y_test)){

    # don't standardize Y unless gaussian
    #std_y <- ifelse(family == "gaussian", T, F)

    # standardize test data based on train data
    if(res$family == "gaussian"){
      test_std <- standardize_dat(X = data$X_test, Y = data$Y_test,
                                     X_train = data$X, Y_train = data$Y,
                                     standardize = res$standardize, std_type = "scale_center", std_y = T)
      xlab <-  paste0("Standardized ", outcome_var)
    }else{
      test_std <- standardize_dat(X = data$X_test, Y = data$Y_test,
                                     X_train = data$X, Y_train = data$Y,
                                     standardize = res$standardize, std_type = "scale_center", std_y = F)
      test_std$Y <- data$Y_test
      xlab <- outcome_var
    }

    if(fix_or_search == 'fixed'){
      res_obj <- res$out
    } else {
      res_obj <- res$out$search_results[[res$best_index]]
    }

    # X variables used in subset fit
    Xsub <- lapply(1:data$D, function(d){
      lapply(1:data$S, function(s){
        test_std$X[[d]][[s]][,rTorch::torch$eq(res_obj$include[[d]], 1)]
      })
    })

    err <- switch(res$family,
                  "gaussian" = test_mse(Y_test = test_std$Y,
                                           X_test = Xsub,
                                           B = res_obj$subset$B,
                                           theta_dict = list('theta' = res_obj$subset$theta,
                                                             'beta' = res_obj$subset$beta)),
                  "multiclass" = test_class(Y_test = test_std$Y,
                                               X_test = Xsub,
                                               B = res_obj$subset$B,
                                               theta_dict = list('theta' = res_obj$subset$theta,
                                                                 'beta' = res_obj$subset$beta)),
                  "poisson" = test_pois(Y_test = test_std$Y,
                                           X_test = Xsub,
                                           B = res_obj$subset$B,
                                           theta_dict = list('theta' = res_obj$subset$theta,
                                                             'beta' = res_obj$subset$beta)),
                  "zip" = test_zip(Y_test = test_std$Y,
                                      X_test = Xsub,
                                      B = res_obj$subset$B,
                                      theta_dict = list('theta' = res_obj$subset$theta,
                                                        'beta' = res_obj$subset$beta,
                                                        'tau' = res_obj$subset$tau))
    )


    # Format data for plotting; y_val, subgroup
    if(res$family == "multiclass"){
      # which.max will give 1 or 2, but generated data has 0 or 1
      # predicted values from Python will start at 0
      truedat <- lapply(1:data$S, FUN = function(s){
        data.frame(y_val = apply(np$array(test_std$Y[[s]]), 1, which.max) - 1,
                   type = "True Value",
                   subgroup = data$sub_vec[[s]])
      }) %>%
        dplyr::bind_rows()
    } else {
      truedat <- lapply(1:data$S, FUN = function(s){
        data.frame(y_val = np$array(test_std$Y[[s]])[,1],
                   type = "True Value",
                   subgroup = data$sub_vec[[s]])
      }) %>%
        dplyr::bind_rows()
    }

    preddat <- lapply(1:data$S, FUN = function(s){
      data.frame(y_val = np$array(err$pred[[s]]),
                 type = "Predicted Value",
                 subgroup = data$sub_vec[[s]])
    }) %>%
      dplyr::bind_rows()

    plotdat <- dplyr::bind_rows(truedat, preddat)


    if(res$family == "multiclass"){
      p <- ggplot(plotdat, aes(x = y_val)) +
        facet_grid(cols = vars(subgroup), rows = vars(type)) +
        geom_bar() +
        theme_classic() +
        labs(x = xlab,
             y = "Count")
    }else{
      p <- ggplot(plotdat, aes(x = y_val)) +
        facet_grid(cols = vars(subgroup), rows = vars(type)) +
        geom_histogram(binwidth = 1) +
        theme_classic() +
        labs(x = xlab,
             y = "Count")

    print(p)

    return(list(err = err, plot = p))

  } # end if non-null test data
  }
  else{
    return(list(err = NULL, plot = NULL))
  }
}
